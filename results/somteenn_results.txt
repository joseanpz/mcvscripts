15 iter

FECHA 	F1 SCORE           	ACCURACY          	RECALL            	PRECISION          
201501	0.30592396109637493	0.9170803844935037	0.5785953177257525	0.20793269230769232
201502	0.28021015761821366	0.9132270663992399	0.5111821086261981	0.19300361881785283
201503	0.31166797180892714	0.9097350585335798	0.5605633802816902	0.2158351409978308 
201504	0.3363957597173145 	0.904892130051656 	0.5876543209876544	0.23564356435643563
201505	0.34126984126984133	0.9168169973942674	0.5205811138014528	0.2538370720188902 
201506	0.3303009097270819 	0.9068975581282226	0.5605700712589073	0.23412698412698413
201507	0.3414634146341463 	0.9090909090909091	0.5658198614318707	0.24451097804391217
201508	0.35928961748633875	0.9095293209876543	0.6173708920187794	0.25337186897880537
201509	0.34818941504178275	0.9124169551791896	0.6082725060827251	0.24390243902439024
201510	0.3576896311760613 	0.9146871245031888	0.6177884615384616	0.2517140058765916 
201511	0.29595827900912647	0.9002032895952689	0.5523114355231143	0.20213713268032057
201512	0.32114882506527415	0.9058226931087566	0.5267665952890792	0.23098591549295774
201601	0.31237322515212984	0.9089117778772952	0.4978448275862069	0.22758620689655173
201602	0.2939150401836969 	0.8912274495932083	0.5203252032520326	0.2048             
201603	0.33710285406569734	0.8934383656509696	0.5743119266055046	0.2385670731707317 
201604	0.35049161364950837	0.9035389108400619	0.5420393559928444	0.258974358974359  
201605	0.33960236432025787	0.8950738495688552	0.5841035120147874	0.23939393939393938
201606	0.31995719636169073	0.8945490749191073	0.5609756097560976	0.22380239520958084
201607	0.32921348314606746	0.9019462921901946	0.5307971014492754	0.23859934853420195
201608	0.3493912122816305 	0.899616107163277 	0.6333973128598849	0.2412280701754386 
201609	0.32446531038080334	0.8950907323395982	0.6308316430020284	0.21839887640449437
201610	0.3217703349282297 	0.9001233045622689	0.604494382022472 	0.21923390383048086
201606	0.31995719636169073	0.8945490749191073	0.5609756097560976	0.22380239520958084
201611	0.2921052631578947 	0.8963989986520315	0.6271186440677966	0.19039451114922812
201612	0.2980342422320863 	0.9109054325955734	0.6638418079096046	0.19215044971381848
201701	0.3239364428498206 	0.8944968804991201	0.6528925619834711	0.21540558963871848
201702	0.29572731637061933	0.8823859536599054	0.6172344689378757	0.19444444444444445
201703	0.33623782675550995	0.896837409384211 	0.6119402985074627	0.23180212014134274
201704	0.3176733780760626 	0.9024624240486089	0.5634920634920635	0.22118380062305296
201705	0.3061611374407583 	0.8839844678659164	0.6026119402985075	0.20520965692503176
201706	0.30206378986866794	0.8835042668128082	0.5886654478976234	0.2031545741324921 
201707	0.32889297197978873	0.8867968386796838	0.6172413793103448	0.22417031934877896
201708	0.3317843866171004 	0.888785769528229 	0.6241258741258742	0.2259493670886076 
201709	0.3414634146341463 	0.9066102997694081	0.6022944550669216	0.23827534039334342
201710	0.3141967067200712 	0.8824023199023199	0.706             	0.20206067544361764
201711	0.30754979157017137	0.8862944934590812	0.6747967479674797	0.19916016796640673

iter 100

FECHA 	F1 SCORE           	ACCURACY          	RECALL             	PRECISION          
201501	0.40053050397877976	0.9522552022816098	0.5050167224080268 	0.33186813186813185
201502	0.3710777626193724 	0.9513353742214715	0.43450479233226835	0.3238095238095238 
201503	0.4128553770086527 	0.9512220168412405	0.4704225352112676 	0.36784140969163   
201504	0.4185022026431718 	0.9465208143421452	0.4691358024691358 	0.37773359840954274
201505	0.4219977553310887 	0.9483864501904189	0.4552058111380145 	0.39330543933054396
201506	0.42779587404994573	0.9487304212472031	0.4679334916864608 	0.394              
201507	0.45154419595314166	0.9504569504569504	0.4896073903002309 	0.4189723320158103 
201508	0.473953013278856  	0.9503279320987654	0.5446009389671361 	0.41952983725135623
201509	0.474120082815735  	0.9524656124263123	0.5571776155717761 	0.4126126126126126 
201510	0.47697031729785055	0.9527682780293928	0.5600961538461539 	0.41532976827094475
201511	0.42871794871794877	0.9485307706523748	0.5085158150851582 	0.37056737588652483
201512	0.40513833992094866	0.9454858281264149	0.43897216274089934	0.3761467889908257 
201601	0.35403726708074534	0.9441110613524407	0.36853448275862066	0.34063745019920316
201602	0.38104089219330856	0.9411036434382738	0.4166666666666667 	0.351027397260274  
201603	0.42797668609492096	0.9405297783933518	0.47155963302752296	0.39176829268292684
201604	0.41659610499576627	0.940817728912558 	0.4400715563506261 	0.3954983922829582 
201605	0.4487291849255039 	0.9462989840348331	0.4731977818853974 	0.4266666666666667 
201606	0.3931947069943289 	0.946735252634199 	0.3902439024390244 	0.3961904761904762 
201607	0.4                	0.9445676274944568	0.4076086956521739 	0.39267015706806285
201608	0.4503190519598906 	0.9507473658417055	0.4740882917466411 	0.4288194444444444 
201609	0.42397137745974955	0.9478289047310434	0.48073022312373226	0.3792             
201610	0.42643923240938164	0.9526158182138453	0.449438202247191  	0.4056795131845842 
201606	0.3931947069943289 	0.946735252634199 	0.3902439024390244 	0.3961904761904762 
201611	0.40909090909090906	0.9499326015790487	0.5084745762711864 	0.34220532319391633
201612	0.4095904095904096 	0.9524346076458753	0.5790960451977402 	0.3168469860896445 
201701	0.455160744500846  	0.948488241881299 	0.5557851239669421 	0.3853868194842407 
201702	0.41576506955177744	0.9393890804136936	0.5390781563126252 	0.3383647798742138 
201703	0.454320987654321  	0.9471839400940014	0.5149253731343284 	0.406480117820324  
201704	0.39656518345042935	0.9381995522865366	0.503968253968254  	0.3268983268983269 
201705	0.4236220472440945 	0.9419922339329583	0.5018656716417911 	0.36648501362397823
201706	0.40481565086531224	0.9380724966726689	0.49177330895795246	0.34398976982097185
201707	0.43203526818515803	0.9401053773438711	0.506896551724138  	0.3764404609475032 
201708	0.43431221020092736	0.9433874709976798	0.49125874125874125	0.389196675900277  
201709	0.4378980891719745 	0.9457340507302076	0.5258126195028681 	0.37517053206002726
201710	0.4342948717948718 	0.9461233211233211	0.542              	0.3622994652406417 
201711	0.41596316193399846	0.9421204745968969	0.5508130081300813 	0.33415536374845867


------------------------------------------------------------------
Muestra  [201702, 201703, 201704, 201705; 201602, 201603, 201604, 201605; 201502, 201503, 201504, 201505] 
entrenamiento  ~ 80% * SMOTEENN  (OVERSAMPLING + EDIT NEARES NEIGHBOUR)
test ~ 20%
validation > 201705

thr 0.59
iter 200
------              	Train                      	Test                       	Validation                 
confusion matrix    	[[14220   276]             	[[4510  178]               	[[201882   7585]           
                    	 [  454 17130]]            	 [ 101  115]]              	 [  4328   4520]]          
confusion matrix pct	[[44.32668329  0.86034913] 	[[91.96574225  3.62969005] 	[[92.47280306  3.47433754] 
                    	 [ 1.41521197 53.39775561]]	 [ 2.05954323  2.34502447]]	 [ 1.98245654  2.07040286]]
f1_score            	0.9791368962560733         	0.45186640471512773        	0.43144179831050444        
accuracy score      	0.9772443890274314         	0.9431076672104405         	0.9454320591805419         
recall score        	0.9741810737033667         	0.5324074074074074         	0.5108499095840868         
precision_score     	0.9841433988279904         	0.3924914675767918         	0.37339942172655927        
{'objective': 'binary:logistic', 'eta': 0.03, 'max_depth': 5, 'eval_metric': 'auc', 'alpha': 30, 'lambda': 1, 'gamma': 0.01, 'max_delta_step': 5, 'min_child_weight': 1}

FECHA 	F1 SCORE           	ACCURACY          	RECALL             	PRECISION          
201501	0.3914680050188206 	0.9487694095278335	0.5217391304347826 	0.3132530120481928 
201502	0.3754789272030652 	0.9483796051937085	0.4696485623003195 	0.3127659574468085 
201503	0.43458475540386804	0.9489628260423085	0.5380281690140845 	0.36450381679389315
201504	0.4575569358178054 	0.946925959688038 	0.5456790123456791 	0.3939393939393939 
201505	0.45873526259378344	0.9493886550410904	0.5181598062953995 	0.4115384615384615 
201506	0.4446620959843291 	0.944838992119856 	0.5391923990498813 	0.37833333333333335
201507	0.45773979107312446	0.9450697450697451	0.5565819861431871 	0.38870967741935486
201508	0.4612476370510397 	0.9450231481481481	0.5727699530516432 	0.3860759493670886 
201509	0.48727984344422703	0.9509684663609993	0.6058394160583942 	0.4075286415711948 
201510	0.47766990291262135	0.9502726684536463	0.5913461538461539 	0.4006514657980456 
201511	0.44717948717948713	0.9501940491591203	0.5304136253041363 	0.38652482269503546
201512	0.4343065693430657 	0.9438558362763741	0.5096359743040685 	0.3783783783783784 
201601	0.3794089609151573 	0.941692789968652 	0.42887931034482757	0.3401709401709402 
201602	0.4168912848158131 	0.9426070038910506	0.4715447154471545 	0.37359098228663445
201603	0.46493301812450744	0.941222299168975 	0.5412844036697247 	0.4074585635359116 
201604	0.46265060240963857	0.9425356467960831	0.5152057245080501 	0.4198250728862974 
201605	0.44234079173838203	0.9446768547767438	0.47504621072088726	0.4138486312399356 
201606	0.4019230769230769 	0.9483945905583672	0.3921200750469043 	0.41222879684418146
201607	0.39332096474953615	0.9462921901946292	0.38405797101449274	0.40304182509505704
201608	0.425891181988743  	0.9500122518990444	0.43570057581573896	0.41651376146788993
201609	0.42359767891682787	0.9517174335709656	0.44421906693711966	0.4048059149722736 
201610	0.40728100113765636	0.954113087898538 	0.40224719101123596	0.41244239631336405
201606	0.4019230769230769 	0.9483945905583672	0.3921200750469043 	0.41222879684418146
201611	0.43879907621247116	0.9532062391681109	0.536723163841808  	0.37109375         
201612	0.41366574330563255	0.9488933601609658	0.632768361581921  	0.30727023319615915
201701	0.4601479046836483 	0.9474484082546792	0.5785123966942148 	0.3819918144611187 
201702	0.43079315707620525	0.9413132365910366	0.5551102204408818 	0.3519695044472681 
201703	0.4933852140077821 	0.9481398868796304	0.5914179104477612 	0.4232309746328438 
201704	0.4260263361735089 	0.9407579149344419	0.5456349206349206 	0.34942820838627703
201705	0.4393241167434716 	0.9421507250970759	0.5335820895522388 	0.3733681462140992 
201706	0.422680412371134  	0.9386205276755657	0.5246800731261426 	0.35388409371146734
201707	0.43231441048034935	0.9395629939562994	0.5120689655172413 	0.37405541561712846
201708	0.44690265486725667	0.9419953596287703	0.5297202797202797 	0.3864795918367347 
201709	0.42572741194486985	0.9423520368946964	0.5315487571701721 	0.3550446998722861 
201710	0.42788461538461536	0.9455128205128205	0.534              	0.356951871657754  
201711	0.43735763097949887	0.9436416184971098	0.5853658536585366 	0.3490909090909091 


------------------------------------------------------------------
Muestra  [201702, 201703, 201704, 201705; 201605, 201606, 201607, 201608; 201508, 201509, 201510, 201511; 201501, 201502] 
entrenamiento  ~ 80% * SMOTEENN  (OVERSAMPLING + EDIT NEARES NEIGHBOUR)
test ~ 20%
validation > 201705


thr 0.62
iter 15
------              	Train                      	Test                       	Validation                 
confusion matrix    	[[18597   350]             	[[6015  148]               	[[203819   5648]           
                    	 [ 3792 19104]]            	 [ 141  132]]              	 [  5218   3630]]          
confusion matrix pct	[[44.44470999  0.8364601 ] 	[[93.45866998  2.29956495] 	[[93.36005313  2.58708747] 
                    	 [ 9.06244772 45.65638219]]	 [ 2.19080174  2.05096333]]	 [ 2.39012436  1.66273504]]
f1_score            	0.9021959858323495         	0.47739602169981915        	0.4005296259516716         
accuracy score      	0.9010109217790312         	0.9550963331261654         	0.950227881730527          
recall score        	0.8343815513626834         	0.4835164835164835         	0.41026220614828207        
precision_score     	0.9820088413693842         	0.4714285714285714         	0.39124811381763314        
{'objective': 'binary:logistic', 'eta': 0.03, 'max_depth': 5, 'eval_metric': 'auc', 'alpha': 30, 'lambda': 1, 'gamma': 0.01, 'max_delta_step': 5, 'min_child_weight': 1}

FECHA 	F1 SCORE           	ACCURACY          	RECALL             	PRECISION          
201501	0.35330261136712754	0.9555297348684906	0.38461538461538464	0.32670454545454547
201502	0.3375796178343949 	0.9560857173018051	0.33865814696485624	0.33650793650793653
201503	0.3878787878787879 	0.9585130416923393	0.36056338028169016	0.419672131147541  
201504	0.4180645161290322 	0.9543198622505824	0.4                	0.43783783783783786
201505	0.3728813559322034 	0.9517939466827019	0.34624697336561744	0.403954802259887  
201506	0.3960149439601494 	0.9528164218309174	0.37767220902612825	0.4162303664921466 
201507	0.42770352369380316	0.9546897546897547	0.4064665127020785 	0.4512820512820513 
201508	0.4406392694063927 	0.9527391975308642	0.45305164319248825	0.4288888888888889 
201509	0.4570737605804111 	0.957986338542154 	0.45985401459854014	0.4543269230769231 
201510	0.4536817102137767 	0.9574822072280248	0.45913461538461536	0.44835680751173707
201511	0.41747572815533973	0.9556459064867862	0.41849148418491483	0.41646489104116224
201512	0.39220183486238536	0.9520057955265779	0.36616702355460384	0.4222222222222222 
201601	0.3456495828367103 	0.9508284818629646	0.3125             	0.38666666666666666
201602	0.36882546652030734	0.9491510435090201	0.34146341463414637	0.4009546539379475 
201603	0.3925925925925926 	0.943213296398892 	0.3889908256880734 	0.39626168224299063
201604	0.4071969696969697 	0.9462291702456622	0.38461538461538464	0.43259557344064387
201605	0.4313326551373347 	0.9522752497225305	0.39186691312384475	0.4796380090497738 
201606	0.3734939759036145 	0.9482286567659504	0.34896810506566606	0.4017278617710583 
201607	0.3857677902621723 	0.9461279461279462	0.37318840579710144	0.3992248062015504 
201608	0.4408396946564886 	0.9521359144000654	0.44337811900191937	0.43833017077798864
201609	0.4343720491029273 	0.9514744005184705	0.4665314401622718 	0.40636042402826855
201610	0.4108352144469526 	0.9540250132112031	0.40898876404494383	0.4126984126984127 
201606	0.3734939759036145 	0.9482286567659504	0.34896810506566606	0.4017278617710583 
201611	0.3389830508474576 	0.9549393414211439	0.3389830508474576 	0.3389830508474576 
201612	0.3985148514851485 	0.9608853118712274	0.4548022598870056 	0.35462555066079293
201701	0.41182466870540263	0.953847384418493 	0.41735537190082644	0.40643863179074446
201702	0.39292364990689005	0.9477270905155135	0.4228456913827655 	0.36695652173913046
201703	0.4139941690962099 	0.9519636740221461	0.39738805970149255	0.43204868154158216
201704	0.37871956717763755	0.9449152542372882	0.4166666666666667 	0.34710743801652894
201705	0.37057728119180633	0.946429986528251 	0.3712686567164179 	0.36988847583643125
201706	0.38230088495575226	0.9453534799968684	0.39488117001828155	0.3704974271012007 
201707	0.39759036144578314	0.9457616612428328	0.39827586206896554	0.39690721649484534
201708	0.40636042402826855	0.9480278422273782	0.4020979020979021 	0.4107142857142857 
201709	0.41219963031423285	0.9511145272867025	0.42638623326959846	0.39892665474060823
201710	0.40317775571002984	0.9541361416361417	0.406              	0.40039447731755423
201711	0.41881100266193433	0.9501825372680256	0.4796747967479675 	0.3716535433070866 


thr 0.72
iter 200
------              	Train                      	Test                       	Validation                 
confusion matrix    	[[18751   196]             	[[6059  104]               	[[204706   4761]           
                    	 [ 1047 21849]]            	 [ 142  131]]              	 [  5063   3785]]          
confusion matrix pct	[[44.81275243  0.46841766] 	[[94.14232443  1.6159105 ] 	[[93.76634679  2.18079381] 
                    	 [ 2.50221064 52.21661927]]	 [ 2.20633934  2.03542573]]	 [ 2.31912603  1.73373337]]
f1_score            	0.9723415144300304         	0.515748031496063          	0.4352075428308612         
accuracy score      	0.9702937169896996         	0.9617775015537601         	0.955000801594027          
recall score        	0.9542714884696016         	0.47985347985347987        	0.42778028933092227        
precision_score     	0.9911090950328872         	0.5574468085106383         	0.4428972618769015         
{'objective': 'binary:logistic', 'eta': 0.03, 'max_depth': 5, 'eval_metric': 'auc', 'alpha': 30, 'lambda': 1, 'gamma': 0.01, 'max_delta_step': 5, 'min_child_weight': 1}


FECHA 	F1 SCORE           	ACCURACY          	RECALL             	PRECISION          
201501	0.3919753086419753 	0.9583817471215802	0.42474916387959866	0.3638968481375358 
201502	0.41017488076311603	0.9608360603821388	0.41214057507987223	0.40822784810126583
201503	0.44565217391304346	0.958102279728897 	0.4619718309859155 	0.4304461942257218 
201504	0.4534313725490196 	0.9548262939329485	0.4567901234567901 	0.45012165450121655
201505	0.416452442159383  	0.9544998997795149	0.3922518159806295 	0.4438356164383562 
201506	0.42028985507246375	0.9533028504718358	0.41330166270783847	0.4275184275184275 
201507	0.47362250879249707	0.9568061568061568	0.4665127020785219 	0.48095238095238096
201508	0.4825986078886311 	0.956983024691358 	0.48826291079812206	0.47706422018348627
201509	0.5157526254375729 	0.9611677739309441	0.537712895377129  	0.49551569506726456
201510	0.5125748502994011 	0.9623809963952306	0.5144230769230769 	0.5107398568019093 
201511	0.4605911330049261 	0.9595268896691924	0.45498783454987834	0.46633416458852867
201512	0.4537037037037037 	0.9572579914878203	0.4197002141327623 	0.49370277078085645
201601	0.3847072879330944 	0.9538737124944021	0.34698275862068967	0.4316353887399464 
201602	0.40944881889763785	0.9535726918995402	0.3699186991869919 	0.45843828715365237
201603	0.4581749049429658 	0.9506578947368421	0.44220183486238535	0.47534516765285995
201604	0.44019138755980863	0.9497509019068888	0.41144901610017887	0.4732510288065844 
201605	0.45369406867845996	0.9551780073422693	0.4029574861367837 	0.5190476190476191 
201606	0.3991031390134529 	0.9555297436322907	0.3339587242026266 	0.4958217270194986 
201607	0.40803382663847776	0.9540116613287345	0.3496376811594203 	0.48984771573604063
201608	0.42826086956521736	0.9570366740178061	0.3781190019193858 	0.49373433583959897
201609	0.4211686879823594 	0.9574692158133506	0.38742393509127787	0.4613526570048309 
201610	0.4180645161290322 	0.9602783160119781	0.36404494382022473	0.4909090909090909 
201606	0.3991031390134529 	0.9555297436322907	0.3339587242026266 	0.4958217270194986 
201611	0.43227665706051877	0.9620643173502792	0.423728813559322  	0.4411764705882353 
201612	0.4552058111380145 	0.9637826961770624	0.5310734463276836 	0.3983050847457627 
201701	0.4543610547667343 	0.9569668852983523	0.4628099173553719 	0.44621513944223107
201702	0.452054794520548  	0.9551030225286619	0.46292585170340683	0.4416826003824092 
201703	0.48282265552460535	0.9556281367003904	0.48507462686567165	0.4805914972273567 
201704	0.4436296975252063 	0.9514710585225455	0.4801587301587302 	0.4122657580919932 
201705	0.4320297951582867 	0.9516601949441319	0.43283582089552236	0.4312267657992565 
201706	0.41740674955595025	0.9486416660142488	0.4296160877513711 	0.4058721934369603 
201707	0.42857142857142855	0.9491709282504261	0.4241379310344828 	0.43309859154929575
201708	0.4307417336907954 	0.9507347254447023	0.42132867132867136	0.4405850091407678 
201709	0.4352720450281426 	0.953727901614143 	0.4435946462715105 	0.427255985267035  
201710	0.44               	0.9572649572649573	0.44               	0.44               
201711	0.4440298507462687 	0.9546699117736538	0.483739837398374  	0.4103448275862069 